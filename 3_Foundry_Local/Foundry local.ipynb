{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee623337",
   "metadata": {},
   "source": [
    "# Foundry local\n",
    "\n",
    "<img src=\"https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/media/architecture/foundry-local-arch.png\">\n",
    "\n",
    "Foundry Local is an on-device AI inference solution offering performance, privacy, customization, and cost advantages. It integrates seamlessly into your existing workflows and applications through an intuitive CLI, SDK, and REST API.\n",
    "\n",
    "## Key features\n",
    "- On-Device Inference: Run models locally on your own hardware, reducing your costs while keeping all your data on your device.\n",
    "- Model Customization: Select from preset models or use your own to meet specific requirements and use cases.\n",
    "- Cost Efficiency: Eliminate recurring cloud service costs by using your existing hardware, making AI more accessible.\n",
    "- Seamless Integration: Connect with your applications through an SDK, API endpoints, or the CLI, with easy scaling to Azure AI Foundry as your needs grow.\n",
    "\n",
    "## Use cases\n",
    "Foundry Local is ideal for scenarios where:\n",
    "- You want to keep sensitive data on your device.\n",
    "- You need to operate in environments with limited or no internet connectivity.\n",
    "- You want to reduce cloud inference costs.\n",
    "- You need low-latency AI responses for real-time applications.\n",
    "- You want to experiment with AI models before deploying to a cloud environment.\n",
    "\n",
    "## Documentation\n",
    "> https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/what-is-foundry-local\n",
    "> https://github.com/microsoft/Foundry-Local/releases\n",
    "> https://github.com/microsoft/Foundry-Local/tree/main/docs\n",
    "\n",
    "## Note\n",
    "\n",
    "Foundry Local is available in preview. Public preview releases provide early access to features that are in active deployment.\n",
    "Features, approaches, and processes can change or have limited capabilities, before General Availability (GA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be3e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install foundry-local-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80582ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from foundry_local import FoundryLocalManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b627423",
   "metadata": {},
   "source": [
    "## List of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47367e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<foundry_local.api.FoundryLocalManager at 0x1207cfb60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager = FoundryLocalManager()\n",
    "manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e10015c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.is_service_running()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e84f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:5273'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.service_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e71bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:5273/v1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manager.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a16e037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models in the catalog: [FoundryModelInfo(alias=phi-4, id=Phi-4-generic-gpu, runtime=webgpu, file_size=8570 MB, license=MIT), FoundryModelInfo(alias=phi-4, id=Phi-4-generic-cpu, runtime=cpu, file_size=10403 MB, license=MIT), FoundryModelInfo(alias=mistral-7b-v0.2, id=mistralai-Mistral-7B-Instruct-v0-2-generic-gpu, runtime=webgpu, file_size=4167 MB, license=apache-2.0), FoundryModelInfo(alias=mistral-7b-v0.2, id=mistralai-Mistral-7B-Instruct-v0-2-generic-cpu, runtime=cpu, file_size=4167 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-gpu, runtime=webgpu, file_size=2211 MB, license=MIT), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-cpu, runtime=cpu, file_size=2590 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-128k, id=Phi-3-mini-128k-instruct-generic-gpu, runtime=webgpu, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-128k, id=Phi-3-mini-128k-instruct-generic-cpu, runtime=cpu, file_size=2600 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-generic-gpu, runtime=webgpu, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-generic-cpu, runtime=cpu, file_size=2590 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-14b, id=deepseek-r1-distill-qwen-14b-generic-gpu, runtime=webgpu, file_size=10516 MB, license=MIT), FoundryModelInfo(alias=deepseek-r1-7b, id=deepseek-r1-distill-qwen-7b-generic-gpu, runtime=webgpu, file_size=5713 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-generic-gpu, runtime=webgpu, file_size=700 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-generic-cpu, runtime=cpu, file_size=822 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-1.5b, id=qwen2.5-1.5b-instruct-generic-gpu, runtime=webgpu, file_size=1546 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-1.5b, id=qwen2.5-1.5b-instruct-generic-cpu, runtime=cpu, file_size=1822 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-0.5b, id=qwen2.5-coder-0.5b-instruct-generic-gpu, runtime=webgpu, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-0.5b, id=qwen2.5-coder-0.5b-instruct-generic-cpu, runtime=cpu, file_size=822 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-generic-gpu, runtime=webgpu, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-generic-cpu, runtime=cpu, file_size=6307 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-1.5b, id=qwen2.5-coder-1.5b-instruct-generic-gpu, runtime=webgpu, file_size=1280 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-1.5b, id=qwen2.5-coder-1.5b-instruct-generic-cpu, runtime=cpu, file_size=1822 MB, license=apache-2.0), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-generic-gpu, runtime=webgpu, file_size=3809 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-generic-gpu, runtime=webgpu, file_size=3225 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-generic-cpu, runtime=cpu, file_size=4628 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-14b, id=qwen2.5-14b-instruct-generic-cpu, runtime=cpu, file_size=11325 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-generic-gpu, runtime=webgpu, file_size=5324 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-generic-cpu, runtime=cpu, file_size=6307 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-14b, id=qwen2.5-coder-14b-instruct-generic-gpu, runtime=webgpu, file_size=9000 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-14b, id=qwen2.5-coder-14b-instruct-generic-cpu, runtime=cpu, file_size=11325 MB, license=apache-2.0)]\n"
     ]
    }
   ],
   "source": [
    "# List available models in the catalog\n",
    "catalog = manager.list_catalog_models()\n",
    "print(f\"Available models in the catalog: {catalog}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfe862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: alias='phi-4' id='Phi-4-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-generic-gpu/versions/1' file_size_mb=8570 prompt_template={'system': '<|system|>\\n{Content}<|im_end|>', 'user': '<|user|>\\n{Content}<|im_end|>', 'assistant': '<|assistant|>\\n{Content}<|im_end|>', 'prompt': '<|user|>\\n{Content}<|im_end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "2: alias='phi-4' id='Phi-4-generic-cpu' version='1' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-generic-cpu/versions/1' file_size_mb=10403 prompt_template={'system': '<|system|>\\n{Content}<|im_end|>', 'user': '<|user|>\\n{Content}<|im_end|>', 'assistant': '<|assistant|>\\n{Content}<|im_end|>', 'prompt': '<|user|>\\n{Content}<|im_end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "3: alias='mistral-7b-v0.2' id='mistralai-Mistral-7B-Instruct-v0-2-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/mistralai-Mistral-7B-Instruct-v0-2-generic-gpu/versions/1' file_size_mb=4167 prompt_template={'prompt': '[INST]\\n{Content}\\n[/INST]', 'assistant': '{Content}</s>'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "4: alias='mistral-7b-v0.2' id='mistralai-Mistral-7B-Instruct-v0-2-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/mistralai-Mistral-7B-Instruct-v0-2-generic-cpu/versions/2' file_size_mb=4167 prompt_template={'system': '<s>', 'user': '[INST]\\n{Content}\\n[/INST]', 'assistant': '{Content}</s>', 'prompt': '[INST]\\n{Content}\\n[/INST]'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "5: alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-generic-gpu/versions/1' file_size_mb=2211 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "6: alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-generic-cpu' version='1' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-generic-cpu/versions/1' file_size_mb=2590 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "7: alias='phi-3-mini-128k' id='Phi-3-mini-128k-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-128k-instruct-generic-gpu/versions/1' file_size_mb=2181 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "8: alias='phi-3-mini-128k' id='Phi-3-mini-128k-instruct-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-128k-instruct-generic-cpu/versions/2' file_size_mb=2600 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "9: alias='phi-3-mini-4k' id='Phi-3-mini-4k-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-4k-instruct-generic-gpu/versions/1' file_size_mb=2181 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "10: alias='phi-3-mini-4k' id='Phi-3-mini-4k-instruct-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-4k-instruct-generic-cpu/versions/2' file_size_mb=2590 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "11: alias='deepseek-r1-14b' id='deepseek-r1-distill-qwen-14b-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-14b-generic-gpu/versions/3' file_size_mb=10516 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "12: alias='deepseek-r1-7b' id='deepseek-r1-distill-qwen-7b-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/deepseek-r1-distill-qwen-7b-generic-gpu/versions/3' file_size_mb=5713 prompt_template={'assistant': '{Content}', 'prompt': '\\\\u003C\\\\uFF5CUser\\\\uFF5C\\\\u003E{Content}\\\\u003C\\\\uFF5CAssistant\\\\uFF5C\\\\u003E'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "13: alias='qwen2.5-0.5b' id='qwen2.5-0.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-0.5b-instruct-generic-gpu/versions/3' file_size_mb=700 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "14: alias='qwen2.5-0.5b' id='qwen2.5-0.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-0.5b-instruct-generic-cpu/versions/3' file_size_mb=822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "15: alias='qwen2.5-1.5b' id='qwen2.5-1.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-1.5b-instruct-generic-gpu/versions/3' file_size_mb=1546 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "16: alias='qwen2.5-1.5b' id='qwen2.5-1.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-1.5b-instruct-generic-cpu/versions/3' file_size_mb=1822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "17: alias='qwen2.5-coder-0.5b' id='qwen2.5-coder-0.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-0.5b-instruct-generic-gpu/versions/3' file_size_mb=528 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "18: alias='qwen2.5-coder-0.5b' id='qwen2.5-coder-0.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-0.5b-instruct-generic-cpu/versions/3' file_size_mb=822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "19: alias='qwen2.5-coder-7b' id='qwen2.5-coder-7b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-7b-instruct-generic-gpu/versions/3' file_size_mb=4843 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "20: alias='qwen2.5-coder-7b' id='qwen2.5-coder-7b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-7b-instruct-generic-cpu/versions/3' file_size_mb=6307 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "21: alias='qwen2.5-coder-1.5b' id='qwen2.5-coder-1.5b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-1.5b-instruct-generic-gpu/versions/3' file_size_mb=1280 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "22: alias='qwen2.5-coder-1.5b' id='qwen2.5-coder-1.5b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-1.5b-instruct-generic-cpu/versions/3' file_size_mb=1822 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "23: alias='phi-4-mini' id='Phi-4-mini-instruct-generic-gpu' version='4' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-instruct-generic-gpu/versions/4' file_size_mb=3809 prompt_template={'system': '<|system|>{Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "24: alias='phi-4-mini-reasoning' id='Phi-4-mini-reasoning-generic-gpu' version='2' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-reasoning-generic-gpu/versions/2' file_size_mb=3225 prompt_template={'system': '<|system|>Your name is Phi, an AI math expert developed by Microsoft. {Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "25: alias='phi-4-mini-reasoning' id='Phi-4-mini-reasoning-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-reasoning-generic-cpu/versions/2' file_size_mb=4628 prompt_template={'system': '<|system|>Your name is Phi, an AI math expert developed by Microsoft. {Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "26: alias='qwen2.5-14b' id='qwen2.5-14b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-14b-instruct-generic-cpu/versions/3' file_size_mb=11325 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "27: alias='qwen2.5-7b' id='qwen2.5-7b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-7b-instruct-generic-gpu/versions/3' file_size_mb=5324 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "28: alias='qwen2.5-7b' id='qwen2.5-7b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-7b-instruct-generic-cpu/versions/3' file_size_mb=6307 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "29: alias='qwen2.5-coder-14b' id='qwen2.5-coder-14b-instruct-generic-gpu' version='3' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-14b-instruct-generic-gpu/versions/3' file_size_mb=9000 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "30: alias='qwen2.5-coder-14b' id='qwen2.5-coder-14b-instruct-generic-cpu' version='3' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/qwen2.5-coder-14b-instruct-generic-cpu/versions/3' file_size_mb=11325 prompt_template={'system': '<|im_start|>system\\n{Content}<|im_end|>', 'user': '<|im_start|>user\\n{Content}<|im_end|>', 'assistant': '<|im_start|>assistant\\n{Content}<|im_end|>', 'prompt': '<|im_start|>user\\n{Content}<|im_end|>\\n<|im_start|>assistant'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, item in enumerate(catalog, start=1):\n",
    "    print(f\"{idx}: {item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5aa7204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>alias</th>\n",
       "      <th>id</th>\n",
       "      <th>version</th>\n",
       "      <th>runtime</th>\n",
       "      <th>uri</th>\n",
       "      <th>file_size_mb</th>\n",
       "      <th>prompt_template</th>\n",
       "      <th>provider</th>\n",
       "      <th>publisher</th>\n",
       "      <th>license</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>phi-4</td>\n",
       "      <td>Phi-4-generic-gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-4-gene...</td>\n",
       "      <td>8570</td>\n",
       "      <td>{'system': '&lt;|system|&gt;\n",
       "{Content}&lt;|im_end|&gt;', '...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>phi-4</td>\n",
       "      <td>Phi-4-generic-cpu</td>\n",
       "      <td>1</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-4-gene...</td>\n",
       "      <td>10403</td>\n",
       "      <td>{'system': '&lt;|system|&gt;\n",
       "{Content}&lt;|im_end|&gt;', '...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>mistral-7b-v0.2</td>\n",
       "      <td>mistralai-Mistral-7B-Instruct-v0-2-generic-gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/mistralai-...</td>\n",
       "      <td>4167</td>\n",
       "      <td>{'prompt': '[INST]\n",
       "{Content}\n",
       "[/INST]', 'assist...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>mistral-7b-v0.2</td>\n",
       "      <td>mistralai-Mistral-7B-Instruct-v0-2-generic-cpu</td>\n",
       "      <td>2</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/mistralai-...</td>\n",
       "      <td>4167</td>\n",
       "      <td>{'system': '&lt;s&gt;', 'user': '[INST]\n",
       "{Content}\n",
       "[/...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>phi-3.5-mini</td>\n",
       "      <td>Phi-3.5-mini-instruct-generic-gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-3.5-mi...</td>\n",
       "      <td>2211</td>\n",
       "      <td>{'prompt': '&lt;|user|&gt;\n",
       "{Content}&lt;|end|&gt;\n",
       "&lt;|assist...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>phi-3.5-mini</td>\n",
       "      <td>Phi-3.5-mini-instruct-generic-cpu</td>\n",
       "      <td>1</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-3.5-mi...</td>\n",
       "      <td>2590</td>\n",
       "      <td>{'prompt': '&lt;|user|&gt;\n",
       "{Content}&lt;|end|&gt;\n",
       "&lt;|assist...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>phi-3-mini-128k</td>\n",
       "      <td>Phi-3-mini-128k-instruct-generic-gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-3-mini...</td>\n",
       "      <td>2181</td>\n",
       "      <td>{'system': '&lt;|system|&gt;\n",
       "{Content}&lt;|end|&gt;', 'use...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>phi-3-mini-128k</td>\n",
       "      <td>Phi-3-mini-128k-instruct-generic-cpu</td>\n",
       "      <td>2</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-3-mini...</td>\n",
       "      <td>2600</td>\n",
       "      <td>{'system': '&lt;|system|&gt;\n",
       "{Content}&lt;|end|&gt;', 'use...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>phi-3-mini-4k</td>\n",
       "      <td>Phi-3-mini-4k-instruct-generic-gpu</td>\n",
       "      <td>1</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-3-mini...</td>\n",
       "      <td>2181</td>\n",
       "      <td>{'system': '&lt;|system|&gt;\n",
       "{Content}&lt;|end|&gt;', 'use...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>phi-3-mini-4k</td>\n",
       "      <td>Phi-3-mini-4k-instruct-generic-cpu</td>\n",
       "      <td>2</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-3-mini...</td>\n",
       "      <td>2590</td>\n",
       "      <td>{'system': '&lt;|system|&gt;\n",
       "{Content}&lt;|end|&gt;', 'use...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>deepseek-r1-14b</td>\n",
       "      <td>deepseek-r1-distill-qwen-14b-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/deepseek-r...</td>\n",
       "      <td>10516</td>\n",
       "      <td>{'assistant': '{Content}', 'prompt': '\\u003C\\u...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>deepseek-r1-7b</td>\n",
       "      <td>deepseek-r1-distill-qwen-7b-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/deepseek-r...</td>\n",
       "      <td>5713</td>\n",
       "      <td>{'assistant': '{Content}', 'prompt': '\\u003C\\u...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>qwen2.5-0.5b</td>\n",
       "      <td>qwen2.5-0.5b-instruct-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-0....</td>\n",
       "      <td>700</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>qwen2.5-0.5b</td>\n",
       "      <td>qwen2.5-0.5b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-0....</td>\n",
       "      <td>822</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>qwen2.5-1.5b</td>\n",
       "      <td>qwen2.5-1.5b-instruct-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-1....</td>\n",
       "      <td>1546</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>qwen2.5-1.5b</td>\n",
       "      <td>qwen2.5-1.5b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-1....</td>\n",
       "      <td>1822</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>qwen2.5-coder-0.5b</td>\n",
       "      <td>qwen2.5-coder-0.5b-instruct-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>528</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>qwen2.5-coder-0.5b</td>\n",
       "      <td>qwen2.5-coder-0.5b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>822</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>qwen2.5-coder-7b</td>\n",
       "      <td>qwen2.5-coder-7b-instruct-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>4843</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>qwen2.5-coder-7b</td>\n",
       "      <td>qwen2.5-coder-7b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>6307</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>qwen2.5-coder-1.5b</td>\n",
       "      <td>qwen2.5-coder-1.5b-instruct-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>1280</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>qwen2.5-coder-1.5b</td>\n",
       "      <td>qwen2.5-coder-1.5b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>1822</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>phi-4-mini</td>\n",
       "      <td>Phi-4-mini-instruct-generic-gpu</td>\n",
       "      <td>4</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-4-mini...</td>\n",
       "      <td>3809</td>\n",
       "      <td>{'system': '&lt;|system|&gt;{Content}&lt;|end|&gt;', 'user...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>phi-4-mini-reasoning</td>\n",
       "      <td>Phi-4-mini-reasoning-generic-gpu</td>\n",
       "      <td>2</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-4-mini...</td>\n",
       "      <td>3225</td>\n",
       "      <td>{'system': '&lt;|system|&gt;Your name is Phi, an AI ...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>phi-4-mini-reasoning</td>\n",
       "      <td>Phi-4-mini-reasoning-generic-cpu</td>\n",
       "      <td>2</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/Phi-4-mini...</td>\n",
       "      <td>4628</td>\n",
       "      <td>{'system': '&lt;|system|&gt;Your name is Phi, an AI ...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>MIT</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>qwen2.5-14b</td>\n",
       "      <td>qwen2.5-14b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-14...</td>\n",
       "      <td>11325</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>qwen2.5-7b</td>\n",
       "      <td>qwen2.5-7b-instruct-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-7b...</td>\n",
       "      <td>5324</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>qwen2.5-7b</td>\n",
       "      <td>qwen2.5-7b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-7b...</td>\n",
       "      <td>6307</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>qwen2.5-coder-14b</td>\n",
       "      <td>qwen2.5-coder-14b-instruct-generic-gpu</td>\n",
       "      <td>3</td>\n",
       "      <td>WebGpuExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>9000</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>qwen2.5-coder-14b</td>\n",
       "      <td>qwen2.5-coder-14b-instruct-generic-cpu</td>\n",
       "      <td>3</td>\n",
       "      <td>CPUExecutionProvider</td>\n",
       "      <td>azureml://registries/azureml/models/qwen2.5-co...</td>\n",
       "      <td>11325</td>\n",
       "      <td>{'system': '&lt;|im_start|&gt;system\n",
       "{Content}&lt;|im_e...</td>\n",
       "      <td>AzureFoundry</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>apache-2.0</td>\n",
       "      <td>chat-completion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                 alias  \\\n",
       "0       1                 phi-4   \n",
       "1       2                 phi-4   \n",
       "2       3       mistral-7b-v0.2   \n",
       "3       4       mistral-7b-v0.2   \n",
       "4       5          phi-3.5-mini   \n",
       "5       6          phi-3.5-mini   \n",
       "6       7       phi-3-mini-128k   \n",
       "7       8       phi-3-mini-128k   \n",
       "8       9         phi-3-mini-4k   \n",
       "9      10         phi-3-mini-4k   \n",
       "10     11       deepseek-r1-14b   \n",
       "11     12        deepseek-r1-7b   \n",
       "12     13          qwen2.5-0.5b   \n",
       "13     14          qwen2.5-0.5b   \n",
       "14     15          qwen2.5-1.5b   \n",
       "15     16          qwen2.5-1.5b   \n",
       "16     17    qwen2.5-coder-0.5b   \n",
       "17     18    qwen2.5-coder-0.5b   \n",
       "18     19      qwen2.5-coder-7b   \n",
       "19     20      qwen2.5-coder-7b   \n",
       "20     21    qwen2.5-coder-1.5b   \n",
       "21     22    qwen2.5-coder-1.5b   \n",
       "22     23            phi-4-mini   \n",
       "23     24  phi-4-mini-reasoning   \n",
       "24     25  phi-4-mini-reasoning   \n",
       "25     26           qwen2.5-14b   \n",
       "26     27            qwen2.5-7b   \n",
       "27     28            qwen2.5-7b   \n",
       "28     29     qwen2.5-coder-14b   \n",
       "29     30     qwen2.5-coder-14b   \n",
       "\n",
       "                                                id version  \\\n",
       "0                                Phi-4-generic-gpu       1   \n",
       "1                                Phi-4-generic-cpu       1   \n",
       "2   mistralai-Mistral-7B-Instruct-v0-2-generic-gpu       1   \n",
       "3   mistralai-Mistral-7B-Instruct-v0-2-generic-cpu       2   \n",
       "4                Phi-3.5-mini-instruct-generic-gpu       1   \n",
       "5                Phi-3.5-mini-instruct-generic-cpu       1   \n",
       "6             Phi-3-mini-128k-instruct-generic-gpu       1   \n",
       "7             Phi-3-mini-128k-instruct-generic-cpu       2   \n",
       "8               Phi-3-mini-4k-instruct-generic-gpu       1   \n",
       "9               Phi-3-mini-4k-instruct-generic-cpu       2   \n",
       "10        deepseek-r1-distill-qwen-14b-generic-gpu       3   \n",
       "11         deepseek-r1-distill-qwen-7b-generic-gpu       3   \n",
       "12               qwen2.5-0.5b-instruct-generic-gpu       3   \n",
       "13               qwen2.5-0.5b-instruct-generic-cpu       3   \n",
       "14               qwen2.5-1.5b-instruct-generic-gpu       3   \n",
       "15               qwen2.5-1.5b-instruct-generic-cpu       3   \n",
       "16         qwen2.5-coder-0.5b-instruct-generic-gpu       3   \n",
       "17         qwen2.5-coder-0.5b-instruct-generic-cpu       3   \n",
       "18           qwen2.5-coder-7b-instruct-generic-gpu       3   \n",
       "19           qwen2.5-coder-7b-instruct-generic-cpu       3   \n",
       "20         qwen2.5-coder-1.5b-instruct-generic-gpu       3   \n",
       "21         qwen2.5-coder-1.5b-instruct-generic-cpu       3   \n",
       "22                 Phi-4-mini-instruct-generic-gpu       4   \n",
       "23                Phi-4-mini-reasoning-generic-gpu       2   \n",
       "24                Phi-4-mini-reasoning-generic-cpu       2   \n",
       "25                qwen2.5-14b-instruct-generic-cpu       3   \n",
       "26                 qwen2.5-7b-instruct-generic-gpu       3   \n",
       "27                 qwen2.5-7b-instruct-generic-cpu       3   \n",
       "28          qwen2.5-coder-14b-instruct-generic-gpu       3   \n",
       "29          qwen2.5-coder-14b-instruct-generic-cpu       3   \n",
       "\n",
       "                    runtime  \\\n",
       "0   WebGpuExecutionProvider   \n",
       "1      CPUExecutionProvider   \n",
       "2   WebGpuExecutionProvider   \n",
       "3      CPUExecutionProvider   \n",
       "4   WebGpuExecutionProvider   \n",
       "5      CPUExecutionProvider   \n",
       "6   WebGpuExecutionProvider   \n",
       "7      CPUExecutionProvider   \n",
       "8   WebGpuExecutionProvider   \n",
       "9      CPUExecutionProvider   \n",
       "10  WebGpuExecutionProvider   \n",
       "11  WebGpuExecutionProvider   \n",
       "12  WebGpuExecutionProvider   \n",
       "13     CPUExecutionProvider   \n",
       "14  WebGpuExecutionProvider   \n",
       "15     CPUExecutionProvider   \n",
       "16  WebGpuExecutionProvider   \n",
       "17     CPUExecutionProvider   \n",
       "18  WebGpuExecutionProvider   \n",
       "19     CPUExecutionProvider   \n",
       "20  WebGpuExecutionProvider   \n",
       "21     CPUExecutionProvider   \n",
       "22  WebGpuExecutionProvider   \n",
       "23  WebGpuExecutionProvider   \n",
       "24     CPUExecutionProvider   \n",
       "25     CPUExecutionProvider   \n",
       "26  WebGpuExecutionProvider   \n",
       "27     CPUExecutionProvider   \n",
       "28  WebGpuExecutionProvider   \n",
       "29     CPUExecutionProvider   \n",
       "\n",
       "                                                  uri  file_size_mb  \\\n",
       "0   azureml://registries/azureml/models/Phi-4-gene...          8570   \n",
       "1   azureml://registries/azureml/models/Phi-4-gene...         10403   \n",
       "2   azureml://registries/azureml/models/mistralai-...          4167   \n",
       "3   azureml://registries/azureml/models/mistralai-...          4167   \n",
       "4   azureml://registries/azureml/models/Phi-3.5-mi...          2211   \n",
       "5   azureml://registries/azureml/models/Phi-3.5-mi...          2590   \n",
       "6   azureml://registries/azureml/models/Phi-3-mini...          2181   \n",
       "7   azureml://registries/azureml/models/Phi-3-mini...          2600   \n",
       "8   azureml://registries/azureml/models/Phi-3-mini...          2181   \n",
       "9   azureml://registries/azureml/models/Phi-3-mini...          2590   \n",
       "10  azureml://registries/azureml/models/deepseek-r...         10516   \n",
       "11  azureml://registries/azureml/models/deepseek-r...          5713   \n",
       "12  azureml://registries/azureml/models/qwen2.5-0....           700   \n",
       "13  azureml://registries/azureml/models/qwen2.5-0....           822   \n",
       "14  azureml://registries/azureml/models/qwen2.5-1....          1546   \n",
       "15  azureml://registries/azureml/models/qwen2.5-1....          1822   \n",
       "16  azureml://registries/azureml/models/qwen2.5-co...           528   \n",
       "17  azureml://registries/azureml/models/qwen2.5-co...           822   \n",
       "18  azureml://registries/azureml/models/qwen2.5-co...          4843   \n",
       "19  azureml://registries/azureml/models/qwen2.5-co...          6307   \n",
       "20  azureml://registries/azureml/models/qwen2.5-co...          1280   \n",
       "21  azureml://registries/azureml/models/qwen2.5-co...          1822   \n",
       "22  azureml://registries/azureml/models/Phi-4-mini...          3809   \n",
       "23  azureml://registries/azureml/models/Phi-4-mini...          3225   \n",
       "24  azureml://registries/azureml/models/Phi-4-mini...          4628   \n",
       "25  azureml://registries/azureml/models/qwen2.5-14...         11325   \n",
       "26  azureml://registries/azureml/models/qwen2.5-7b...          5324   \n",
       "27  azureml://registries/azureml/models/qwen2.5-7b...          6307   \n",
       "28  azureml://registries/azureml/models/qwen2.5-co...          9000   \n",
       "29  azureml://registries/azureml/models/qwen2.5-co...         11325   \n",
       "\n",
       "                                      prompt_template      provider  \\\n",
       "0   {'system': '<|system|>\n",
       "{Content}<|im_end|>', '...  AzureFoundry   \n",
       "1   {'system': '<|system|>\n",
       "{Content}<|im_end|>', '...  AzureFoundry   \n",
       "2   {'prompt': '[INST]\n",
       "{Content}\n",
       "[/INST]', 'assist...  AzureFoundry   \n",
       "3   {'system': '<s>', 'user': '[INST]\n",
       "{Content}\n",
       "[/...  AzureFoundry   \n",
       "4   {'prompt': '<|user|>\n",
       "{Content}<|end|>\n",
       "<|assist...  AzureFoundry   \n",
       "5   {'prompt': '<|user|>\n",
       "{Content}<|end|>\n",
       "<|assist...  AzureFoundry   \n",
       "6   {'system': '<|system|>\n",
       "{Content}<|end|>', 'use...  AzureFoundry   \n",
       "7   {'system': '<|system|>\n",
       "{Content}<|end|>', 'use...  AzureFoundry   \n",
       "8   {'system': '<|system|>\n",
       "{Content}<|end|>', 'use...  AzureFoundry   \n",
       "9   {'system': '<|system|>\n",
       "{Content}<|end|>', 'use...  AzureFoundry   \n",
       "10  {'assistant': '{Content}', 'prompt': '\\u003C\\u...  AzureFoundry   \n",
       "11  {'assistant': '{Content}', 'prompt': '\\u003C\\u...  AzureFoundry   \n",
       "12  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "13  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "14  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "15  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "16  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "17  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "18  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "19  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "20  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "21  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "22  {'system': '<|system|>{Content}<|end|>', 'user...  AzureFoundry   \n",
       "23  {'system': '<|system|>Your name is Phi, an AI ...  AzureFoundry   \n",
       "24  {'system': '<|system|>Your name is Phi, an AI ...  AzureFoundry   \n",
       "25  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "26  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "27  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "28  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "29  {'system': '<|im_start|>system\n",
       "{Content}<|im_e...  AzureFoundry   \n",
       "\n",
       "    publisher     license             task  \n",
       "0   Microsoft         MIT  chat-completion  \n",
       "1   Microsoft         MIT  chat-completion  \n",
       "2   Microsoft  apache-2.0  chat-completion  \n",
       "3   Microsoft  apache-2.0  chat-completion  \n",
       "4   Microsoft         MIT  chat-completion  \n",
       "5   Microsoft         MIT  chat-completion  \n",
       "6   Microsoft         MIT  chat-completion  \n",
       "7   Microsoft         MIT  chat-completion  \n",
       "8   Microsoft         MIT  chat-completion  \n",
       "9   Microsoft         MIT  chat-completion  \n",
       "10  Microsoft         MIT  chat-completion  \n",
       "11  Microsoft         MIT  chat-completion  \n",
       "12  Microsoft  apache-2.0  chat-completion  \n",
       "13  Microsoft  apache-2.0  chat-completion  \n",
       "14  Microsoft  apache-2.0  chat-completion  \n",
       "15  Microsoft  apache-2.0  chat-completion  \n",
       "16  Microsoft  apache-2.0  chat-completion  \n",
       "17  Microsoft  apache-2.0  chat-completion  \n",
       "18  Microsoft  apache-2.0  chat-completion  \n",
       "19  Microsoft  apache-2.0  chat-completion  \n",
       "20  Microsoft  apache-2.0  chat-completion  \n",
       "21  Microsoft  apache-2.0  chat-completion  \n",
       "22  Microsoft         MIT  chat-completion  \n",
       "23  Microsoft         MIT  chat-completion  \n",
       "24  Microsoft         MIT  chat-completion  \n",
       "25  Microsoft  apache-2.0  chat-completion  \n",
       "26  Microsoft  apache-2.0  chat-completion  \n",
       "27  Microsoft  apache-2.0  chat-completion  \n",
       "28  Microsoft  apache-2.0  chat-completion  \n",
       "29  Microsoft  apache-2.0  chat-completion  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for idx, item in enumerate(catalog, start=1):\n",
    "    data.append({\n",
    "        \"index\": idx,\n",
    "        \"alias\": item.alias,\n",
    "        \"id\": item.id,\n",
    "        \"version\": item.version,\n",
    "        \"runtime\": str(item.runtime),\n",
    "        \"uri\": item.uri,\n",
    "        \"file_size_mb\": item.file_size_mb,\n",
    "        \"prompt_template\": item.prompt_template,\n",
    "        \"provider\": item.provider,\n",
    "        \"publisher\": item.publisher,\n",
    "        \"license\": item.license,\n",
    "        \"task\": item.task\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28bcd51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi-4-generic-gpu\n",
      "Phi-4-generic-cpu\n",
      "mistralai-Mistral-7B-Instruct-v0-2-generic-gpu\n",
      "mistralai-Mistral-7B-Instruct-v0-2-generic-cpu\n",
      "Phi-3.5-mini-instruct-generic-gpu\n",
      "Phi-3.5-mini-instruct-generic-cpu\n",
      "Phi-3-mini-128k-instruct-generic-gpu\n",
      "Phi-3-mini-128k-instruct-generic-cpu\n",
      "Phi-3-mini-4k-instruct-generic-gpu\n",
      "Phi-3-mini-4k-instruct-generic-cpu\n",
      "deepseek-r1-distill-qwen-14b-generic-gpu\n",
      "deepseek-r1-distill-qwen-7b-generic-gpu\n",
      "qwen2.5-0.5b-instruct-generic-gpu\n",
      "qwen2.5-0.5b-instruct-generic-cpu\n",
      "qwen2.5-1.5b-instruct-generic-gpu\n",
      "qwen2.5-1.5b-instruct-generic-cpu\n",
      "qwen2.5-coder-0.5b-instruct-generic-gpu\n",
      "qwen2.5-coder-0.5b-instruct-generic-cpu\n",
      "qwen2.5-coder-7b-instruct-generic-gpu\n",
      "qwen2.5-coder-7b-instruct-generic-cpu\n",
      "qwen2.5-coder-1.5b-instruct-generic-gpu\n",
      "qwen2.5-coder-1.5b-instruct-generic-cpu\n",
      "Phi-4-mini-instruct-generic-gpu\n",
      "Phi-4-mini-reasoning-generic-gpu\n",
      "Phi-4-mini-reasoning-generic-cpu\n",
      "qwen2.5-14b-instruct-generic-cpu\n",
      "qwen2.5-7b-instruct-generic-gpu\n",
      "qwen2.5-7b-instruct-generic-cpu\n",
      "qwen2.5-coder-14b-instruct-generic-gpu\n",
      "qwen2.5-coder-14b-instruct-generic-cpu\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(catalog)):\n",
    "    model = catalog[idx].id\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "777c87bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in the catalog = 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of models in the catalog =\", len(catalog))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95451a7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1fa45ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = \"phi-3.5-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07e39c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model info:\n",
      "alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-generic-gpu/versions/1' file_size_mb=2211 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n"
     ]
    }
   ],
   "source": [
    "# Download and load a model\n",
    "model_info = manager.download_model(alias)\n",
    "model_info = manager.load_model(alias)\n",
    "print(f\"Model info:\\n{model_info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1d83fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in cache:\n",
      "[FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-generic-gpu, runtime=webgpu, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-gpu, runtime=webgpu, file_size=2211 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-generic-gpu, runtime=webgpu, file_size=3225 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache:\\n{local_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef88026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models running in the service:\n",
      "[FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-gpu, runtime=webgpu, file_size=2211 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List loaded models\n",
    "loaded = manager.list_loaded_models()\n",
    "print(f\"Models running in the service:\\n{loaded}\")\n",
    "\n",
    "# Unload a model\n",
    "manager.unload_model(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aeb996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The number 3.1415 is a decimal approximation of the mathematical constant pi (), which represents the ratio of a circle' extruded circumference to its diameter. Pi is an irrational number, meaning it has an infinite number of non-repeating decimals. The value 3.1415 is often used as a close estimate for practical calculations, but for more precise work, more digits of pi are used. For example, in scientific and engineering contexts, pi is often approximated to several decimal places, such as 3.14159 or even more accurately with the help of computer algorithms."
     ]
    }
   ],
   "source": [
    "# Streaming\n",
    "\n",
    "alias = \"phi-3.5-mini\"\n",
    "\n",
    "manager = FoundryLocalManager(alias)\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=manager.endpoint,\n",
    "    api_key=manager.api_key  # API key is not required for local usage\n",
    ")\n",
    "\n",
    "# Set the model to use and generate a streaming response\n",
    "stream = client.chat.completions.create(model=manager.get_model_info(alias).id,\n",
    "                                        messages=[{\n",
    "                                            \"role\": \"user\",\n",
    "                                            \"content\": \"What is 3.1415?\"\n",
    "                                        }],\n",
    "                                        stream=True)\n",
    "\n",
    "# Print the streaming response\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cfb167e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chat.id.2594', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=' The capital of Canada is Ottawa. Located in the province of Ontario, Ottawa is not only the political center of the country but also home to many national institutions, including Parliament Hill, where the Senate and House of Commons meet. The city was chosen as the capital by Queen Victoria in 1857 due to its strategic location near the geographical and linguistic divide between English-speaking Canada West and French-speaking Canada East. Ottawa is the fourth largest city in the country and is known for its rich history and diverse culture.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], name=None, tool_call_id=None), delta={'role': 'assistant', 'content': ' The capital of Canada is Ottawa. Located in the province of Ontario, Ottawa is not only the political center of the country but also home to many national institutions, including Parliament Hill, where the Senate and House of Commons meet. The city was chosen as the capital by Queen Victoria in 1857 due to its strategic location near the geographical and linguistic divide between English-speaking Canada West and French-speaking Canada East. Ottawa is the fourth largest city in the country and is known for its rich history and diverse culture.', 'name': None, 'tool_call_id': None, 'function_call': None, 'tool_calls': []}, finish_details=None)], created=1749742623, model=None, object='chat.completion', service_tier=None, system_fingerprint=None, usage=None, CreatedAt='2025-06-12T15:37:03+00:00', StreamEvent=None, IsDelta=False, Successful=True, error=None, HttpStatusCode=0, HeaderValues=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No streaming mode\n",
    "resp = client.chat.completions.create(\n",
    "    model=manager.get_model_info(alias).id,\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the capital of Canada?\"\n",
    "    }],\n",
    ")\n",
    "\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36411bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The capital of Canada is Ottawa. Located in the province of Ontario, Ottawa is not only the political center of the country but also home to many national institutions, including Parliament Hill, where the Senate and House of Commons meet. The city was chosen as the capital by Queen Victoria in 1857 due to its strategic location near the geographical and linguistic divide between English-speaking Canada West and French-speaking Canada East. Ottawa is the fourth largest city in the country and is known for its rich history and diverse culture.\n"
     ]
    }
   ],
   "source": [
    "print(resp.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3ad7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in cache:\n",
      "[FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-generic-gpu, runtime=webgpu, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-gpu, runtime=webgpu, file_size=2211 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-generic-gpu, runtime=webgpu, file_size=3225 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache:\\n{local_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e15d300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in cache = 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of models in cache =\", len(local_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2e3c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models running in the service:\n",
      "[FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-gpu, runtime=webgpu, file_size=2211 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List loaded models\n",
    "loaded = manager.list_loaded_models()\n",
    "print(f\"Models running in the service:\\n{loaded}\")\n",
    "\n",
    "# Unload a model\n",
    "manager.unload_model(alias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcee53e0",
   "metadata": {},
   "source": [
    "## Rest API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d261682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Azure is a cloud computing platform and set of services offered by Microsoft. It provides a range of solutions including those for computing, storage, networking, databases, analytics, artificial intelligence, and Internet of Things (IoT). Azure allows individuals, businesses, and organizations to build, deploy, and manage applications and workloads through a global network of Microsoft-managed data centers. With Azure, users can benefit from the flexibility of the cloud without having to manage the underlying infrastructure, enabling them to focus on their applications and business logic.\n"
     ]
    }
   ],
   "source": [
    "alias = \"mistralai-Mistral-7B-Instruct-v0-2-generic-cpu\"\n",
    "\n",
    "manager = FoundryLocalManager(alias)\n",
    "url = manager.endpoint + \"/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": manager.get_model_info(alias).id,\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is Azure?\",\n",
    "    }]\n",
    "}\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "177055da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': None,\n",
       " 'choices': [{'delta': {'role': 'assistant',\n",
       "    'content': ' Azure is a cloud computing platform and set of services offered by Microsoft. It provides a range of solutions including those for computing, storage, networking, databases, analytics, artificial intelligence, and Internet of Things (IoT). Azure allows individuals, businesses, and organizations to build, deploy, and manage applications and workloads through a global network of Microsoft-managed data centers. With Azure, users can benefit from the flexibility of the cloud without having to manage the underlying infrastructure, enabling them to focus on their applications and business logic.',\n",
       "    'name': None,\n",
       "    'tool_call_id': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': []},\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': ' Azure is a cloud computing platform and set of services offered by Microsoft. It provides a range of solutions including those for computing, storage, networking, databases, analytics, artificial intelligence, and Internet of Things (IoT). Azure allows individuals, businesses, and organizations to build, deploy, and manage applications and workloads through a global network of Microsoft-managed data centers. With Azure, users can benefit from the flexibility of the cloud without having to manage the underlying infrastructure, enabling them to focus on their applications and business logic.',\n",
       "    'name': None,\n",
       "    'tool_call_id': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': []},\n",
       "   'index': 0,\n",
       "   'finish_reason': 'stop',\n",
       "   'finish_details': None,\n",
       "   'logprobs': None}],\n",
       " 'usage': None,\n",
       " 'system_fingerprint': None,\n",
       " 'service_tier': None,\n",
       " 'created': 1749742708,\n",
       " 'CreatedAt': '2025-06-12T15:38:28+00:00',\n",
       " 'id': 'chat.id.2595',\n",
       " 'StreamEvent': None,\n",
       " 'IsDelta': False,\n",
       " 'Successful': True,\n",
       " 'error': None,\n",
       " 'HttpStatusCode': 0,\n",
       " 'HeaderValues': None,\n",
       " 'object': 'chat.completion'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97681de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models in cache:\n",
      "[FoundryModelInfo(alias=phi-3-mini-4k, id=Phi-3-mini-4k-instruct-generic-gpu, runtime=webgpu, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=mistral-7b-v0.2, id=mistralai-Mistral-7B-Instruct-v0-2-generic-cpu, runtime=cpu, file_size=4167 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-generic-gpu, runtime=webgpu, file_size=2211 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini-reasoning, id=Phi-4-mini-reasoning-generic-gpu, runtime=webgpu, file_size=3225 MB, license=MIT)]\n"
     ]
    }
   ],
   "source": [
    "# List models in cache\n",
    "local_models = manager.list_cached_models()\n",
    "print(f\"Models in cache:\\n{local_models}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb490f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alias='phi-3-mini-4k' id='Phi-3-mini-4k-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3-mini-4k-instruct-generic-gpu/versions/1' file_size_mb=2181 prompt_template={'system': '<|system|>\\n{Content}<|end|>', 'user': '<|user|>\\n{Content}<|end|>', 'assistant': '<|assistant|>\\n{Content}<|end|>', 'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "alias='mistral-7b-v0.2' id='mistralai-Mistral-7B-Instruct-v0-2-generic-cpu' version='2' runtime=<ExecutionProvider.CPU: 'CPUExecutionProvider'> uri='azureml://registries/azureml/models/mistralai-Mistral-7B-Instruct-v0-2-generic-cpu/versions/2' file_size_mb=4167 prompt_template={'system': '<s>', 'user': '[INST]\\n{Content}\\n[/INST]', 'assistant': '{Content}</s>', 'prompt': '[INST]\\n{Content}\\n[/INST]'} provider='AzureFoundry' publisher='Microsoft' license='apache-2.0' task='chat-completion'\n",
      "\n",
      "alias='phi-3.5-mini' id='Phi-3.5-mini-instruct-generic-gpu' version='1' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-3.5-mini-instruct-generic-gpu/versions/1' file_size_mb=2211 prompt_template={'prompt': '<|user|>\\n{Content}<|end|>\\n<|assistant|>', 'assistant': '<|assistant|>\\n{Content}<|end|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n",
      "alias='phi-4-mini-reasoning' id='Phi-4-mini-reasoning-generic-gpu' version='2' runtime=<ExecutionProvider.WEBGPU: 'WebGpuExecutionProvider'> uri='azureml://registries/azureml/models/Phi-4-mini-reasoning-generic-gpu/versions/2' file_size_mb=3225 prompt_template={'system': '<|system|>Your name is Phi, an AI math expert developed by Microsoft. {Content}<|end|>', 'user': '<|user|>{Content}<|end|>', 'assistant': '<|assistant|>{Content}<|end|>', 'prompt': '<|user|>{Content}<|end|><|assistant|>'} provider='AzureFoundry' publisher='Microsoft' license='MIT' task='chat-completion'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(local_models)):\n",
    "    print(local_models[idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe7f7f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models in cache = 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of models in cache =\", len(local_models))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7184eeb0",
   "metadata": {},
   "source": [
    "## CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66353390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=Description:\n",
      "  Foundry Local CLI: Run AI models on your device.\n",
      "  \n",
      "   Getting started:\n",
      "  \n",
      "     1. To view available models: foundry model list\n",
      "     2. To run a model: foundry model run <model>\n",
      "  \n",
      "     EXAMPLES:\n",
      "         foundry model run phi-3-mini-4k\n",
      "\n",
      "Usage:\n",
      "  foundry [command] [options]\n",
      "\n",
      "Options:\n",
      "  -?, -h, --help  Show help and usage information\n",
      "  --version       Show version information\n",
      "\n",
      "Commands:\n",
      "  model    Discover, run and manage models\n",
      "  cache    Manage the local cache\n",
      "  service  Manage the local model inference service\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!foundry -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "621d6d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=0.3.9267.42993\n"
     ]
    }
   ],
   "source": [
    "!foundry --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e54e917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=Alias                          Device     Task               File Size    License      Model ID            \n",
      "-----------------------------------------------------------------------------------------------\n",
      "phi-4                          GPU        chat-completion    8.37 GB      MIT          Phi-4-generic-gpu   \n",
      "                               CPU        chat-completion    10.16 GB     MIT          Phi-4-generic-cpu   \n",
      "--------------------------------------------------------------------------------------------------------\n",
      "mistral-7b-v0.2                GPU        chat-completion    4.07 GB      apache-2.0   mistralai-Mistral-7B-Instruct-v0-2-generic-gpu\n",
      "                               CPU        chat-completion    4.07 GB      apache-2.0   mistralai-Mistral-7B-Instruct-v0-2-generic-cpu\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "phi-3.5-mini                   GPU        chat-completion    2.16 GB      MIT          Phi-3.5-mini-instruct-generic-gpu\n",
      "                               CPU        chat-completion    2.53 GB      MIT          Phi-3.5-mini-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "phi-3-mini-128k                GPU        chat-completion    2.13 GB      MIT          Phi-3-mini-128k-instruct-generic-gpu\n",
      "                               CPU        chat-completion    2.54 GB      MIT          Phi-3-mini-128k-instruct-generic-cpu\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "phi-3-mini-4k                  GPU        chat-completion    2.13 GB      MIT          Phi-3-mini-4k-instruct-generic-gpu\n",
      "                               CPU        chat-completion    2.53 GB      MIT          Phi-3-mini-4k-instruct-generic-cpu\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "deepseek-r1-14b                GPU        chat-completion    10.27 GB     MIT          deepseek-r1-distill-qwen-14b-generic-gpu\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "deepseek-r1-7b                 GPU        chat-completion    5.58 GB      MIT          deepseek-r1-distill-qwen-7b-generic-gpu\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-0.5b                   GPU        chat-completion    0.68 GB      apache-2.0   qwen2.5-0.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    0.80 GB      apache-2.0   qwen2.5-0.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-1.5b                   GPU        chat-completion    1.51 GB      apache-2.0   qwen2.5-1.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    1.78 GB      apache-2.0   qwen2.5-1.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-0.5b             GPU        chat-completion    0.52 GB      apache-2.0   qwen2.5-coder-0.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    0.80 GB      apache-2.0   qwen2.5-coder-0.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-7b               GPU        chat-completion    4.73 GB      apache-2.0   qwen2.5-coder-7b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    6.16 GB      apache-2.0   qwen2.5-coder-7b-instruct-generic-cpu\n",
      "----------------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-1.5b             GPU        chat-completion    1.25 GB      apache-2.0   qwen2.5-coder-1.5b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    1.78 GB      apache-2.0   qwen2.5-coder-1.5b-instruct-generic-cpu\n",
      "------------------------------------------------------------------------------------------------------------------------------\n",
      "phi-4-mini                     GPU        chat-completion    3.72 GB      MIT          Phi-4-mini-instruct-generic-gpu\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "phi-4-mini-reasoning           GPU        chat-completion    3.15 GB      MIT          Phi-4-mini-reasoning-generic-gpu\n",
      "                               CPU        chat-completion    4.52 GB      MIT          Phi-4-mini-reasoning-generic-cpu\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-14b                    CPU        chat-completion    11.06 GB     apache-2.0   qwen2.5-14b-instruct-generic-cpu\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-7b                     GPU        chat-completion    5.20 GB      apache-2.0   qwen2.5-7b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    6.16 GB      apache-2.0   qwen2.5-7b-instruct-generic-cpu\n",
      "----------------------------------------------------------------------------------------------------------------------\n",
      "qwen2.5-coder-14b              GPU        chat-completion    8.79 GB      apache-2.0   qwen2.5-coder-14b-instruct-generic-gpu\n",
      "                               CPU        chat-completion    11.06 GB     apache-2.0   qwen2.5-coder-14b-instruct-generic-cpu\n"
     ]
    }
   ],
   "source": [
    "!foundry model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93981275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=Alias                          Device     Task               File Size    License      Model ID            \n",
      "phi-4-mini-reasoning           GPU        chat-completion    3 GB         MIT          Phi-4-mini-reasoning-generic-gpu\n"
     ]
    }
   ],
   "source": [
    "!foundry model info phi-4-mini-reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71c01724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b=Restarting service...\n",
      " Service is stopped.\n",
      " Service is Started on http://localhost:5273, PID 36129!\n"
     ]
    }
   ],
   "source": [
    "!foundry service restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffe21281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?1h\u001b= Service is stopped.\n"
     ]
    }
   ],
   "source": [
    "!foundry service stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef3dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
